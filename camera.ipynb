{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a VideoCapture object to access the camera (usually 0 for the default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "counter=0\n",
    "while True:\n",
    "   \n",
    "    \n",
    "    # Read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Display the frame in a window\n",
    "    \n",
    "    cv2.imwrite('dataset/rei/camera_%s.jpg'%counter, frame)   ## number one\n",
    "    thename='dataset/rei/camera_%s.jpg'%counter\n",
    "    cv2.imshow('Camera Feed.   press q to quit', frame)\n",
    "    image = face_recognition.load_image_file(thename)\n",
    "    counter+=1\n",
    "    # Press 'q' to exit the loop and close the window\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for adding more data, first go to number one then add new folder of the person (add the folder on computer too.) then run it like usual. also go to modify head and change num classes too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## machine learning model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transforms_pred = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(root='dataset/', transform=data_transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Load the pre-trained ResNet-34 model\n",
    "model = models.resnet34(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modify class head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "in_features = model.fc.in_features\n",
    "num_classes = 4  # Number of classes in your dataset     change this if you add more data \n",
    "\n",
    "# Replace the last fully connected layer\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, num_classes),\n",
    "    nn.Sigmoid()  # For binary classification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define your data transformations, model, loss, and optimizer here\n",
    "\n",
    "# Set the number of training epochs, batch size, and other hyperparameters\n",
    "num_epochs = 15\n",
    "batch_size = 16\n",
    "\n",
    "# Create data loaders\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # Print every 10 mini-batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], Loss: {running_loss / 10:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'fine_tuned_resnet34.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"dataset/gang/camera_56.jpg\")\n",
    "image2 = transforms_pred(image)\n",
    "image2 = image2.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Ensure that your model is in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(image2)\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    _, predicted_class = torch.max(probabilities, 1)\n",
    "    predicted_label=dataset.classes[predicted_class[0].item()]\n",
    "\n",
    "    # Add a label\n",
    "    plt.title(\"%s\"%predicted_label)\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # Show the image with the label\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display in video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VideoCapture object to access the camera (usually 0 for the default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "counter=0\n",
    "cv2.destroyAllWindows()\n",
    "cv2.namedWindow(\"Video Feed\", cv2.WINDOW_NORMAL)\n",
    "while True:\n",
    "   \n",
    "    \n",
    "    # Read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    image2 = transforms_pred(pil_image)\n",
    "    image2 = image2.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "    # Ensure that your model is in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image2)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        _, predicted_class = torch.max(probabilities, 1)\n",
    "        predicted_label=dataset.classes[predicted_class[0].item()]\n",
    "    \n",
    "    # Display the frame in a window\n",
    "    dalabel= predicted_label+\"   press q to quit\"\n",
    "    cv2.imshow(dalabel, frame)\n",
    "    # Press 'q' to exit the loop and close the window\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
